{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg6827/.local/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:211: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  _openssl_assert(lib.SSL_CTX_use_certificate(ctx, self._cert._x509) == 1)  # type: ignore\n",
      "/home/mg6827/.local/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:186: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  value=certificate_digest(self._cert._x509),  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from syft.util import get_root_data_path\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [(\"User-agent\", \"Mozilla/5.0\")]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# duet1.store.pandas\n",
    "\n",
    "in_dim = 4\n",
    "out_dim = 3\n",
    "n_samples = 150\n",
    "\n",
    "class SyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(32, 64, 3, 1) \n",
    "        self.dropout = self.torch_ref.nn.Dropout2d(0.25)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(147456, 128)\n",
    "        self.fc2 = self.torch_ref.nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.torch_ref.nn.functional.relu(self.conv1(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.conv2(x))\n",
    "        x = self.torch_ref.nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.torch_ref.flatten(x, 1)\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        output = self.torch_ref.nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# def train(iterations, model, torch_ref, optim, data_ptr, target_ptr):\n",
    "#     print('training started')\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     for i in range(iterations):\n",
    "#         print('iteration: ', i)\n",
    "#         optim.zero_grad()\n",
    "#         print('zerograd done')\n",
    "#         output = model(data_ptr)\n",
    "\n",
    "#         # nll_loss = negative log-liklihood loss\n",
    "#         loss = torch_ref.nn.functional.nll_loss(output, target_ptr.long())\n",
    "\n",
    "#         loss_item = loss.item()\n",
    "\n",
    "#         loss_value = loss_item.get(\n",
    "#             reason=\"To evaluate training progress\", request_block=True, timeout_secs=5\n",
    "#         )\n",
    "\n",
    "#         if i % 1 == 0:\n",
    "#             print(\"Epoch\", i, \"loss\", loss_value)\n",
    "\n",
    "#         losses.append(loss_value)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         optim.step()\n",
    "\n",
    "#     return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, torch_ref, optim, training_data_loader):\n",
    "    print('training started')\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, data_pointers in enumerate(training_data_loader):\n",
    "\n",
    "            optim.zero_grad()\n",
    "            data_ptr, target_ptr = data_pointers[0], data_pointers[1]\n",
    "            data_ptr_reshape = torch_ref.unsqueeze(data_ptr,0)\n",
    "            target_ptr_reshape = torch_ref.unsqueeze(target_ptr, 0)\n",
    "\n",
    "            output_ptr = model(data_ptr_reshape)\n",
    "            loss = torch_ref.nn.functional.nll_loss(output_ptr, target_ptr_reshape)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss_item = loss.item().get(\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=3,\n",
    "                delete_obj=False,\n",
    "                verbose=False )\n",
    "            epoch_loss += loss_item\n",
    "        print('loss', epoch, epoch_loss)\n",
    "#             print(epoch, batch_idx, loss.item)\n",
    "\n",
    "#             if batch_idx % 1 == 0:\n",
    "#                 loss_item = loss.item().get(\n",
    "#                     reason=\"To evaluate training progress\",\n",
    "#                     request_block=True,\n",
    "#                     timeout_secs=3,\n",
    "#                     delete_obj=False,\n",
    "#                     verbose=False\n",
    "#                     )\n",
    "#                 print(f\"epoch {epoch}, batch_idx {batch_idx}, loss {loss_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.proxy.torch.TensorPointer object at 0x151e3acec6d0> <syft.proxy.torch.TensorPointer object at 0x151e3acec070> <syft.proxy.syft.lib.python.IntPointer object at 0x151e3acecac0>\n",
      "result 1897\n",
      "result 1897\n"
     ]
    }
   ],
   "source": [
    "data_ptr1 = duet1.store[\"x_train\"]\n",
    "target_ptr1 = duet1.store[\"y_train\"]\n",
    "train_num = duet1.store[\"train_num\"]\n",
    "# target_vals = target_ptr1.get(\n",
    "#             request_block=True,\n",
    "#             reason=\"To write the training loop\",\n",
    "#             timeout_secs=30,\n",
    "#             delete_obj=False,\n",
    "#             verbose=True )\n",
    "# print('target_val', target_vals, target_vals.shape)\n",
    "    \n",
    "print(data_ptr1, target_ptr1, train_num)\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "class DatasetFromPointer(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 X_tensorpointer,\n",
    "                 y_tensorpointer,\n",
    "                 datanum_pointer,\n",
    "                 ):\n",
    "        super(DatasetFromPointer, self).__init__()\n",
    "        self.X_tensorpointer = X_tensorpointer\n",
    "        self.y_tensorpointer = y_tensorpointer\n",
    "        self.datanum_pointer = datanum_pointer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.X_tensorpointer[index]\n",
    "        target = self.y_tensorpointer[index]\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        result = self.datanum_pointer.get(\n",
    "        request_block=True,\n",
    "        reason=\"To write the training loop\",\n",
    "        timeout_secs=30,\n",
    "        delete_obj=False,\n",
    "        verbose=True )\n",
    "        print('result',result)\n",
    "        return result\n",
    "        \n",
    "        \n",
    "def batch_idx_fn(batch):\n",
    "  return batch[0]\n",
    "\n",
    "train_set = DatasetFromPointer(data_ptr1, target_ptr1, train_num)\n",
    "training_data_loader = DataLoader(dataset=train_set, \n",
    "                                batch_size=4, shuffle=True,\n",
    "                                  collate_fn=batch_idx_fn)\n",
    "\n",
    "# num_workers=4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SyNet(torch)\n",
    "remote_model1 = base_model.send(duet1)\n",
    "remote_torch1 = duet1.torch\n",
    "\n",
    "args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"no_cuda\": False,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 42, # the meaning of life\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashsahs <syft.proxy.syft.lib.python.BoolPointer object at 0x151e3ac6e2b0>\n",
      "False\n",
      "Data Owner device is cpu\n"
     ]
    }
   ],
   "source": [
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch1.cuda.is_available()\n",
    "print('hashsahs', has_cuda_ptr)\n",
    "has_cuda = bool(has_cuda_ptr.get(\n",
    "    request_block=True,\n",
    "    reason=\"To run test and inference locally\",\n",
    "    timeout_secs=5,  # change to something slower\n",
    "))\n",
    "print(has_cuda)\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and has_cuda\n",
    "# use_cuda = has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch1.manual_seed(args[\"seed\"])\n",
    "\n",
    "device = remote_torch1.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Data Owner device is {device.type.get()}\")\n",
    "\n",
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_model1.cuda(device)\n",
    "else:\n",
    "    remote_model1.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: <syft.proxy.syft.lib.python.ListPointer object at 0x151e3ac6e880>\n",
      "optim: <syft.proxy.torch.optim.AdamPointer object at 0x151e3ac6e5e0>\n",
      "training started\n",
      "result 1897\n",
      "loss 0 1671.1908284680612\n",
      "result 1897\n"
     ]
    }
   ],
   "source": [
    "params = remote_model1.parameters()\n",
    "optim = remote_torch1.optim.Adam(params=params, lr=0.01)\n",
    "print(\"params:\", params)\n",
    "print(\"optim:\", optim)\n",
    "\n",
    "epochs = 20\n",
    "losses = train(epochs, remote_model1, remote_torch1, optim, training_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 31\n",
      "0 (<syft.proxy.torch.TensorPointer object at 0x15533dbe8fa0>, <syft.proxy.torch.TensorPointer object at 0x15533e787ac0>)\n",
      "1 (<syft.proxy.torch.TensorPointer object at 0x15533e787910>, <syft.proxy.torch.TensorPointer object at 0x15533e787bb0>)\n",
      "2 (<syft.proxy.torch.TensorPointer object at 0x15533dbe8eb0>, <syft.proxy.torch.TensorPointer object at 0x15533dbe8c40>)\n",
      "3 (<syft.proxy.torch.TensorPointer object at 0x15533e787370>, <syft.proxy.torch.TensorPointer object at 0x1553e74e1250>)\n",
      "4 (<syft.proxy.torch.TensorPointer object at 0x15533e787ac0>, <syft.proxy.torch.TensorPointer object at 0x15533f31c0d0>)\n",
      "5 (<syft.proxy.torch.TensorPointer object at 0x15533e787b50>, <syft.proxy.torch.TensorPointer object at 0x15533e787310>)\n",
      "6 (<syft.proxy.torch.TensorPointer object at 0x15533e787730>, <syft.proxy.torch.TensorPointer object at 0x1553e74e1250>)\n",
      "7 (<syft.proxy.torch.TensorPointer object at 0x15533e7878b0>, <syft.proxy.torch.TensorPointer object at 0x15533dbe8eb0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data_pointers in enumerate(training_data_loader):\n",
    "    print(batch_idx, data_pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
