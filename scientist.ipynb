{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg6827/.local/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:211: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  _openssl_assert(lib.SSL_CTX_use_certificate(ctx, self._cert._x509) == 1)  # type: ignore\n",
      "/home/mg6827/.local/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:186: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  value=certificate_digest(self._cert._x509),  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from syft.util import get_root_data_path\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [(\"User-agent\", \"Mozilla/5.0\")]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# duet1.store.pandas\n",
    "\n",
    "in_dim = 4\n",
    "out_dim = 3\n",
    "n_samples = 150\n",
    "\n",
    "class SyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(32, 64, 3, 1) \n",
    "        self.dropout = self.torch_ref.nn.Dropout2d(0.25)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(147456, 128)\n",
    "        self.fc2 = self.torch_ref.nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.torch_ref.nn.functional.relu(self.conv1(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.conv2(x))\n",
    "        x = self.torch_ref.nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.torch_ref.flatten(x, 1)\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        output = self.torch_ref.nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# def train(iterations, model, torch_ref, optim, data_ptr, target_ptr):\n",
    "#     print('training started')\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     for i in range(iterations):\n",
    "#         print('iteration: ', i)\n",
    "#         optim.zero_grad()\n",
    "#         print('zerograd done')\n",
    "#         output = model(data_ptr)\n",
    "\n",
    "#         # nll_loss = negative log-liklihood loss\n",
    "#         loss = torch_ref.nn.functional.nll_loss(output, target_ptr.long())\n",
    "\n",
    "#         loss_item = loss.item()\n",
    "\n",
    "#         loss_value = loss_item.get(\n",
    "#             reason=\"To evaluate training progress\", request_block=True, timeout_secs=5\n",
    "#         )\n",
    "\n",
    "#         if i % 1 == 0:\n",
    "#             print(\"Epoch\", i, \"loss\", loss_value)\n",
    "\n",
    "#         losses.append(loss_value)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         optim.step()\n",
    "\n",
    "#     return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, torch_ref, optim, training_data_loader):\n",
    "    print('training started')\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, data_pointers in enumerate(training_data_loader):\n",
    "\n",
    "            optim.zero_grad()\n",
    "            data_ptr, target_ptr = data_pointers[0], data_pointers[1]\n",
    "            data_ptr_reshape = torch_ref.unsqueeze(data_ptr,0)\n",
    "            target_ptr_reshape = torch_ref.unsqueeze(target_ptr, 0)\n",
    "\n",
    "            output_ptr = model(data_ptr_reshape)\n",
    "            loss = torch_ref.nn.functional.nll_loss(output_ptr, target_ptr_reshape)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss_item = loss.item().get(\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=3,\n",
    "                delete_obj=False,\n",
    "                verbose=False )\n",
    "            epoch_loss += loss_item\n",
    "        print('loss', epoch, epoch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.proxy.torch.TensorPointer object at 0x151e3acec6d0> <syft.proxy.torch.TensorPointer object at 0x151e3acec070> <syft.proxy.syft.lib.python.IntPointer object at 0x151e3acecac0>\n",
      "result 1897\n",
      "result 1897\n"
     ]
    }
   ],
   "source": [
    "data_ptr1 = duet1.store[\"x_train\"]\n",
    "target_ptr1 = duet1.store[\"y_train\"]\n",
    "train_num = duet1.store[\"train_num\"]\n",
    "# target_vals = target_ptr1.get(\n",
    "#             request_block=True,\n",
    "#             reason=\"To write the training loop\",\n",
    "#             timeout_secs=30,\n",
    "#             delete_obj=False,\n",
    "#             verbose=True )\n",
    "# print('target_val', target_vals, target_vals.shape)\n",
    "    \n",
    "print(data_ptr1, target_ptr1, train_num)\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "class DatasetFromPointer(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 X_tensorpointer,\n",
    "                 y_tensorpointer,\n",
    "                 datanum_pointer,\n",
    "                 ):\n",
    "        super(DatasetFromPointer, self).__init__()\n",
    "        self.X_tensorpointer = X_tensorpointer\n",
    "        self.y_tensorpointer = y_tensorpointer\n",
    "        self.datanum_pointer = datanum_pointer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.X_tensorpointer[index]\n",
    "        target = self.y_tensorpointer[index]\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        result = self.datanum_pointer.get(\n",
    "        request_block=True,\n",
    "        reason=\"To write the training loop\",\n",
    "        timeout_secs=30,\n",
    "        delete_obj=False,\n",
    "        verbose=True )\n",
    "        print('result',result)\n",
    "        return result\n",
    "        \n",
    "        \n",
    "def batch_idx_fn(batch):\n",
    "  return batch[0]\n",
    "\n",
    "train_set = DatasetFromPointer(data_ptr1, target_ptr1, train_num)\n",
    "training_data_loader = DataLoader(dataset=train_set, \n",
    "                                batch_size=4, shuffle=True,\n",
    "                                  collate_fn=batch_idx_fn)\n",
    "\n",
    "# num_workers=4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SyNet(torch)\n",
    "remote_model1 = base_model.send(duet1)\n",
    "remote_torch1 = duet1.torch\n",
    "\n",
    "args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"no_cuda\": False,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 42, # the meaning of life\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashsahs <syft.proxy.syft.lib.python.BoolPointer object at 0x151e3ac6e2b0>\n",
      "False\n",
      "Data Owner device is cpu\n"
     ]
    }
   ],
   "source": [
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch1.cuda.is_available()\n",
    "print('hashsahs', has_cuda_ptr)\n",
    "has_cuda = bool(has_cuda_ptr.get(\n",
    "    request_block=True,\n",
    "    reason=\"To run test and inference locally\",\n",
    "    timeout_secs=5,  # change to something slower\n",
    "))\n",
    "print(has_cuda)\n",
    "\n",
    "use_cuda = not args[\"no_cuda\"] and has_cuda\n",
    "# use_cuda = has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch1.manual_seed(args[\"seed\"])\n",
    "\n",
    "device = remote_torch1.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Data Owner device is {device.type.get()}\")\n",
    "\n",
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_model1.cuda(device)\n",
    "else:\n",
    "    remote_model1.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: <syft.proxy.syft.lib.python.ListPointer object at 0x151e3ac6e880>\n",
      "optim: <syft.proxy.torch.optim.AdamPointer object at 0x151e3ac6e5e0>\n",
      "training started\n",
      "result 1897\n",
      "loss 0 1671.1908284680612\n",
      "result 1897\n"
     ]
    }
   ],
   "source": [
    "params = remote_model1.parameters()\n",
    "optim = remote_torch1.optim.Adam(params=params, lr=0.01)\n",
    "print(\"params:\", params)\n",
    "print(\"optim:\", optim)\n",
    "\n",
    "epochs = 20\n",
    "losses = train(epochs, remote_model1, remote_torch1, optim, training_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Averaging Model Updates - Aggregator\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight updates (diff) from the remote models\n",
    "remote_model1_updates = remote_model1.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "remote_model2_updates = remote_model2.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "#Average the weights\n",
    "from collections import OrderedDict\n",
    "avg_updates = OrderedDict()\n",
    "avg_updates[\"params\"] = (\n",
    "    remote_model1_updates[\"params\"] + remote_model2_updates[\"params\"]\n",
    ") / 2\n",
    "\n",
    "#Load aggregated weights into the model\n",
    "combined_model = SyNet(torch)\n",
    "combined_model.load_state_dict(avg_updates)\n",
    "\n",
    "#This combined model will be later used for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_model1.get(\n",
    "#     request_block=True,\n",
    "#     reason=\"test evaluation\",\n",
    "#     timeout_secs=5\n",
    "#     ).save(f\"/scratch/mg6827/federated_models/model1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "test_dir = '/scratch/mg6827/data/xray_data/test/'\n",
    "classes = os.listdir(test_dir)\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(100),             # resize shortest side\n",
    "        transforms.CenterCrop(100),         # crop longest side\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_set = ImageFolder(test_dir, transform=test_transform)\n",
    "test_data_loader = DataLoader(dataset=test_set, batch_size=4, shuffle=True)\n",
    "\n",
    "local_model = combined_model\n",
    "\n",
    "assert local_model.is_local, \"model is remote try .get()\"\n",
    "num_classes = 4\n",
    "\n",
    "print('test started')\n",
    "local_model = local_model.cuda()\n",
    "correct_pred = 0\n",
    "total_pred = 0\n",
    "for batch_idx, data in enumerate(test_data_loader):\n",
    "    data_local, target_local = data[0], data[1]\n",
    "    data_local = torch.unsqueeze(data_local, 0).cuda()\n",
    "    output_local = local_model(output_local)\n",
    "    output_local = output_local.cpu().numpy().reshape(num_classes)\n",
    "    target_local = target_local.numpy()\n",
    "    predicted_class = np.argmax(output_local)\n",
    "    expected_class = target_local[0]\n",
    "    if(predicted_class == expected_class):\n",
    "        correct_pred += 1\n",
    "    total_pred += 1\n",
    "\n",
    "print(\"Accuracy: \", correct_pred/total_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
